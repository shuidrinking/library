#### 1、人工智能基础概念

|<div style="width:2rem;">中文名</div>|<div style="width:2rem;">英文名</div>|解释说明|
|:---|:---|:---|
|人工智能|Artificial Intelligence, AI|研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。通俗讲，就是让机器能够像人一样思考、学习和解决问题。|
|大语言模型|Large Language Model, LLM|基于海量文本数据训练的、拥有巨大参数量的深度学习模型，能够理解和生成自然语言文本，是ChatGPT等应用的核心。|
|智能体|Agent|在人工智能领域，Agent是一种能自主感知、决策并行动的智能实体，具备自主性、反应能力和目标导向性。它通过模块化架构实现感知、决策与执行。现代智能体系统通常采用模块化设计，各司其职又紧密协同。|
|令牌|token|在AI领域，特指构成自然语言的最小单元，也是AI处理文字的最小单元，一个汉字是或一个英文单词就是一个token。|
|变换器|Transformer|“变换”的全过程是：接收输入原表示序列 --&gt;一系列变换计算--&gt;输出新表示序列。<br><label style="font-weight:bold;">目标</label>：对输入序列进行权重计算，得到一个上下文感知的、全新的输出序列。<br><label style="font-weight:bold;">本质</label>：通过注意力机制，让序列内部所有元素充分交互，从而为每个元素生成一个深度融合了全局上下文信息的新表示。<br><label style="font-weight:bold;">计算权重</label>：注意力机制中的核心计算功能，同时也就是实现变换的核心手段：没有权重计算，那个革命性的“变换”就无法发生，是实现“变换”的核心机制与引擎。|
|自注意力机制|Self-Attention machanism|Self-Attention被翻译为“自注意力”，但是对其理解不能“所见即所得”，该文字无法概述其结构和功能。<br><label style="font-weight:bold;">本质</label>：transformer的变换由多个小变换叠加运算实现，“自注意力”是“变换”的原子计算单元，变换依靠“权重计算”实现，因此，“自注意力”也可以被视为实现“计算权重”的最小、最完整的逻辑功能闭环。<br><label style="font-weight:bold;">谁在注意</label>:“Self”指的是Input的内容序列中的每一个“Token本身”，并不是指AI。<br><label style="font-weight:bold;">注意的是谁</label>：序列中的所有Token (包括自己)<br><label style="font-weight:bold;">如何注意（如何计算权重）</label>：通过计算Query (Q) 和 Key (K) 的相似度，得出注意力权重。这个过程是模型执行的，但主体是Token。<br><label style="font-weight:bold;">注意的结果</label>：token被“Self-Attention"经过权重计算后，生成一个融合了全局上下文信息的新向量来代表当前Token。<br>在Transformer架构中，“自注意力”是一个基础的构建块（Building Block），就像堆积木中的基础单位，<label style="font-weight:bold;">多头自注意力</label>只不过是多个独立的“最小单元”并行运行。|
|预训练嵌入|Pre-trained Embeddings|Pre-trained Embeddings是一套通过预训练得到的、富含语义知识的、以文件形式被持久化存储的参数集合，是模型的“初始词汇库和常识库”。<br>通过一套复杂的计算逻辑和算法（如Word2Vec的CBOW/Skip-gram，或GloVe的共现矩阵分解）在海量文本数据上训练出来的。这个训练过程包含了定义模型结构、损失函数和优化算法，这套算法会输出一个最终结果——一个巨大的矩阵，其中每一行对应一个词的向量表示。这个矩阵本身，就是Pre-trained Embeddings。<br>本质上就是一个“键值对”数据库：<br>键（Key）：词的索引或单词本身。<br>值（Value）：该词对应的固定长度的向量。<br>当我们在代码中加载一个Pre-trained Embeddings文件（例如Google News的Word2Vec向量.bin文件）时，就是将这个“数据库”加载到了内存中。当我们查询单词“cat”时，这个“数据库”会直接返回一个300维的向量给我们。<br>Pre-trained Embeddings提供了token的语义先验，没有它们，模型就是从零开始的随机符号，权重计算也失去了意义的基础。<br>它为权重计算提供高质量、有意义的“原材料”，权重计算是在模型内部对这些嵌入进行动态混合、筛选和精炼的过程。<br><label style="font-weight:bold;">与“计算权重”的关系：</label><br>（1）权重计算严重依赖于Pre-trained Embeddings的质量。如果预训练嵌入很差（例如，“猫”和“汽车”的向量很接近），那么无论权重计算多么精巧，模型都很难学到正确的语义。<br>（2）Pre-trained Embeddings的潜力通过权重计算被释放。再好的预训练嵌入，如果没有强大的权重计算机制（如自注意力），也无法被灵活地组合和理解，无法完成复杂的任务（如机器翻译、阅读理解）。|
|嵌入查找表|Embedding Table|Embedding Table是模型启动时读取Pre-trained Embeddings文件并将其加载到内存中之后的工作副本的名称，本质上是Pre-trained Embeddings在内存中的复制品，冻结模式下两者完全一致，而在微调（fine-tune） 模式下，Embedding Table 这个副本会通过梯度下降被更新，从而适应新的任务，此时会与Pre-trained Embeddings出现差异。|
|计算权重||<label style="font-weight:bold;">谁计算权重</label>：自注意力。这是在模型前向传播过程中动态发生的数学操作。模型根据当前的输入上下文，计算出一系列权重，用以衡量不同部分信息的重要性。<br><label style="font-weight:bold;">作用</label>：<br>（1）动态上下文建模：决定在当前上下文中，哪些词是重要的。例如，在“我吃了苹果，它很甜”中，权重计算会让模型知道“它”应该更关注“苹果”而不是“我”。<br>（2）信息融合：通过加权求和，将多个输入向量的信息融合成一个新的、更丰富的向量。<br>权重决定了信息融合的比例，是Transformer能够理解复杂上下文关系的根本原因。<br>没有“计算权重”，模型就无法知道在生成每个词的新表示时应该重点关注上下文中的哪些部分，所谓的“变换”就会退化为一种静态的、与上下文无关的简单映射，从而完全失去了Transformer的强大能力。|
|张量|tensor|张量是一个多维数组，可以是标量、向量、矩阵或更高维度的数据结构。|
|transfomer编码器/解码器||整个Transformer编码器/解码器层 则是在这个“Self-Attention”的基础上，增加了前馈神经网络、残差连接和层归一化等组件，以增强其表达能力和训练稳定性。封装的更大的block，即：更大的计算单元|
|归一化权重|Softmax|“计算权重”的过程是：“计算关联 --&gt; 归一化为权重 --&gt;加权求和”。其中归一化是通过Softmax函数实现的，该函数将这些关联强度归一化为概率分布，即得到最终的注意力权重。这一步确保了权重的可解释性和稳定性（所有权重之和为1）|
|模态|modal|模式+状态，即AI能够接收的信息类型，如：文本、图像、视频、音频。|
|跨模态转换||NL2SQL、NL2API等本质上都属于“文本模态转文本模态”，也可理解为“文本模态”转换为“代码模态”。|
|检索增强生成|RAG 即：Retrieval Augmented Generation|其核心流程包括：1) 检索：从外部知识库中查找相关文本片段；2) 生成：将检索结果与用户查询输入给大语言模型生成回答。RAG利用Embedding模型将文本转为向量，通过语义匹配实现高效检索，提供更准确、实时的回答。|
|微调|Fine-tuning|对特定领域用专业数据训练模型，让模型成为某个专业领域的“专属AI”，训练ai成为某领域的专家。|
|上下文长度|Context Length|AI能记住对话的长度，决定它能记住多少上下文，与计算量成平方关系。|
|提示词|Prompt|语义明确的指令或文案。|
|提示工程|Prompt Engineering|设计和优化输入给AI模型（尤其是LLM）的指令或问题（即提示），以引导其产生更准确、更符合期望的输出结果。|
|模型的幻觉|Hallucination|AI有编造虚假信息的能力，模型越强大，虚假信息越逼真|
|强化学习|Reinforcement Learning, RL|一种机器学习类型，智能体通过与环境互动并获得奖励或惩罚来学习采取最优行动策略。类似于训练宠物。|
|基于人类反馈的强化学习|RLHF 即：Reinforcement Learning for Human Feedback|人工反馈答案的置信度，使AI学习改进输出对齐人类期望。|
|机器学习|Machine Learning, ML|人工智能的一个核心分支。它利用算法使计算机能够从数据中“学习”规律，而无需进行明确的程序编码。模型通过经验自动改进。|
|深度学习|Deep Learning|机器学习的一个子领域，它使用包含多个层（“深度”）的神经网络模型。它在图像识别、语音识别等领域取得了突破性进展。|
|神经网络|Neural Network|一种受人脑结构启发的计算模型，由大量互连的节点（神经元）组成。通过调整神经元之间的连接强度来处理信息。|
|自然语言处理	|Natural Language Processing, NLP|使计算机能够理解、解释和生成人类语言的技术。应用包括机器翻译、情感分析和聊天机器人。|
|计算机视觉|Computer Vision, CV|让计算机能够“看到”和理解图像与视频内容的技术。应用包括人脸识别、自动驾驶和医疗影像分析。|
|数据挖掘|Data Mining|从大量数据中通过算法自动发现隐藏的、有价值的模式和知识的过程。|
|算法|Algorithm|解决特定问题或执行任务的一系列明确的、有限的步骤和规则。是AI和计算机程序的“食谱”。|
|大数据|Big Data|指规模巨大、结构复杂、无法用传统数据处理工具进行捕捉、管理和处理的数据集合。是训练AI模型的“燃料”。|
|监督学习|Supervised Learning|一种机器学习类型，其训练数据包含“标签”（正确答案）。模型学习从输入到输出的映射关系。例如，用带标签的猫狗图片训练一个分类模型。|
|无监督学习|Unsupervised Learning|一种机器学习类型，其训练数据没有标签。模型自行发现数据中的内在结构或模式，如聚类分析。|
|特征工程|Feature Engineering|将原始数据转换为能更好地表示预测模型的潜在问题的特征的过程，以提高模型性能。|
|过拟合|Overfitting|指模型在训练数据上表现得太好，以至于学习了训练数据中的噪声和细节，导致在新数据上泛化能力差。|
|生成式人工智能|Generative AI|一类能够生成全新内容（如文本、图像、音乐、代码）的AI模型，例如GPT系列和Stable Diffusion。|
|人工神经网络|Artificial Neural Network, ANN|为与生物神经网络区分，特指在计算机上实现的人工神经网络模型，是深度学习的基礎。|
|卷积神经网络|Convolutional Neural Network, CNN|一种专门用于处理网格状数据（如图像）的神经网络，通过卷积层来有效提取空间特征。|
|循环神经网络|Recurrent Neural Network, RNN|一种用于处理序列数据（如文本、时间序列）的神经网络，具有“记忆”功能，能考虑之前的信息。|
|迁移学习|Transfer Learning|一种机器学习方法，将一个领域（任务）上训练好的模型知识，迁移到另一个相关领域（任务）中，以节省训练时间和数据。|
|知识图谱|Knowledge Graph|一种以图的形式组织和表示知识的结构，由节点（实体）和边（关系）组成，用于增强机器的理解和推理能力。|
|嵌入|Embedding|一种将高维、离散的数据（如词语、类别）映射到低维、连续的向量空间的技术，从而捕捉其语义关系。|
|损失函数|Loss Function|用于衡量模型预测值与真实值之间差异的函数。模型训练的目标就是最小化这个损失函数。|
|梯度下降|Gradient Descent|一种优化算法，通过迭代地调整模型参数，沿着损失函数梯度（下降最快）的方向寻找最小值。|
|准确率|Accuracy|分类模型预测正确的样本数占总样本数的比例，是最直观的评估指标之一。|
|精确率与召回率|Precision & Recall|用于评估分类模型性能的一对指标。精确率关注“预测为正的样本中多少是真的正”，召回率关注“真正的正样本中多少被预测对了”。|
|欠拟合|Underfitting|指模型过于简单，未能捕捉到训练数据中的基本规律，导致在训练数据和未知数据上表现都不佳。|
|伦理人工智能	|Ethical AI|指在设计、开发、部署和使用人工智能系统时，遵循道德原则，确保其公平、可问责、透明且符合人类价值观的领域。|
|可解释人工智能|Explainable AI, XAI|旨在使AI模型的决策过程和结果对人类而言是可理解和可解释的技术与方法，增加AI的透明度与信任度。|
|联邦学习|Federated Learning|一种分布式机器学习技术，允许在多个本地设备或服务器上训练模型，而无需将数据集中到一起，保护数据隐私。|
|具身人工智能|Embodied AI|指拥有物理身体（如机器人）或存在于模拟环境中的AI，通过与真实或虚拟环境的交互来学习和完成任务。|
|人工智能对齐|AI Alignment|研究如何确保人工智能系统的目标和行为与人类的价值观和意图保持一致，防止其产生有害或不受控制的行为。|